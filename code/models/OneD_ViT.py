import torch.nn as nn
import torch
import torch.nn.functional as F
from einops import rearrange

# generated by Claude 3 Sonnet


class VisionTransformerSonnet(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(VisionTransformerSonnet, self).__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes

        # Embedding layer
        self.embedding = nn.Linear(input_dim, 768)

        # Transformer encoder
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8, dim_feedforward=2048)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)

        # Classification head
        self.classifier = nn.Linear(768, num_classes)
        self.activation_func = nn.Sigmoid()

    def forward(self, x):
        # Reshape input to (batch_size, sequence_length, input_dim)
        x = rearrange(x, "b n -> b 1 n")
        # Embed input
        x = self.embedding(x)
        # Apply transformer encoder
        x = self.transformer_encoder(x)
        # Take the representation from the first token
        x = x[:, 0]
        # Classify
        x = self.classifier(x)
        x = self.activation_func(x)
        return x


class VisionTransformerOpus(nn.Module):
    def init(self, input_size=40000, num_classes=5, dim=128, depth=6, heads=8, mlp_dim=256, dropout=0.1):
        super().init()
        self.input_size = input_size
        self.num_classes = num_classes
        self.dim = dim

        self.pos_embedding = nn.Parameter(torch.randn(1, input_size + 1, dim))
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(dropout)

        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout), num_layers=depth)

        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))

    def forward(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, -1, 1).transpose(1, 2)

        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding[:, : (x.size(1))]
        x = self.dropout(x)

        x = self.transformer(x)
        x = x[:, 0]

        x = self.mlp_head(x)
        return x
